<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>
			Decoding Language: A Deep Dive into Part of Speech Tagging with Dynamic Algorithms
        </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../projects.css" />
		<noscript><link rel="stylesheet" href="../noscript.css" /></noscript>
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<!-- <a href="../index.html" class="logo">Massively</a> -->
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="../../index.html">Back to Projects</a></li>
							<!-- <li class="active"><a href="../generic.html">Generic Page</a></li>
							<li><a href="../elements.html">Elements Reference</a></li> -->
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">6th March, 2024</span>
									<h1>Decoding Language: A Deep Dive into Part of Speech Tagging with Dynamic Algorithms</h1>
									<p>Comparing the accuracy of part-of-speech tagging using the Eager, Viterbi, and Individually Most Probable Tags algorithms across English, Korean, and Swedish.</p>
									<p>By Emma Horton</p>
								</header>
								
								<body>
									<div class="container">
					
										<section class="section introduction">
											<h2>Introduction</h2>
											<p>What’s the difference between “run” as in “I run every day” and “run” as in “a run in my stocking”? Humans understand these nuances effortlessly, but teaching a computer to do the same is a complex problem. The key lies in Part-of-Speech (POS) tagging, the process of identifying the grammatical role of each word. This fundamental task underpins countless Natural Language Processing (NLP) applications, from machine translation to sentiment analysis. In this article, we’ll take a deep dive into POS tagging, comparing the performance of three different algorithms — the Eager, Viterbi, and Individually Most Probable Tags algorithms — across the diverse linguistic landscapes of English, Korean, and Swedish. I’ll begin by discussing the data used in my experiments, then explore each of the algorithms in detail. Following that, I’ll present and analyse the results for each language, and finally, reflect on the key challenges and learnings from this project. So what are you waiting for? Let’s get started!</p>
										</section>
										<section class="section data preparation">
											<h2>Diving into Data: Setting the Stage for POS Tagging</h2>
											<p>The first step was to gather the right data. To do this, I turned to the <a href="https://universaldependencies.org">Universal Dependancies Treebanks</a>, a rich source of linguistically annotated text. For this project, I selected datasets from the Universal Dependencies Treebank: approximately 47,000 sentences each for English and Korean, and 12,200 sentences for Swedish. These datasets were chosen to explore the challenges of POS tagging across typologically diverse languages.</p> 

											<p>To preprocess the data I firstly, I used the conllu Python package to parse and divide the data into training and testing sets splitting them into sets of an 80/20 split. Then, I structured each sentence as a list of word-tag pairs, including start and end sentence markers, to capture boundary effects crucial for accurate transition probability calculations. Next, the transition and emission probabilities were calculated. I calculated transmission probabilities by tallying the frequency of POS bigrams, applying Witten-Bell smoothing to handle potential data sparsity. Emission probabilities followed, based on how frequently each word appeared with a particular tag. Finally, I calculated the probability of each POS tag starting a sentence. This preprocessing was essential for accurate algorithm training and provided a solid foundation for our analysis.</p>
										</section>
									
										<section class="section algorithms">
											<h2>Unpacking the Algorithms</h2>
											<h3>The Eager Approach</h3>
											<p>I started with the Eager algorithm—a straightforward, no-frills approach to POS tagging. This method handles one word at a time, determining the most likely tag by considering only the tag of the preceding word and the word itself. In my script, this process unfolds within the <code>eager_algorithm</code> function. For each word, I sift through every possible tag, crunch the numbers by multiplying each tag’s emission probability with the transition probability from the previous tag, and then select the tag with the highest score. It’s all about making the best choice, one step at a time.</p>
											<p>
												For those curious about the math behind it, the Eager algorithm relies on a straightforward formula to make these tagging decisions. The formula looks like this:
											</p>
											<p>
												\[\hat {t}_i = \argmax _{t_i} P(t_i \mid \hat {t}_{i-1}) \cdot P(w_i \mid t_i)\]
											</p>
												
											<p>Let me break it down:</p>

											<ul>
												<li><strong>\(\hat{t}_i\)</strong>: This represents the most likely POS tag for the \(i\)-th word in the sentence. It’s what the algorithm is trying to determine.</li>
												<li><strong>\(P(t_i \mid \hat{t}_{i-1})\)</strong>: This is the <strong>transition probability</strong>—it tells us how likely the current tag \(t_i\) is, given that the previous word was tagged as \(\hat{t}_{i-1}\). Essentially, it’s about how well the current tag flows from the previous one.</li>
												<li><strong>\(P(w_i \mid t_i)\)</strong>: This is the <strong>emission probability</strong>, which measures how likely it is that the current word \(w_i\) would be associated with the tag \(t_i\). This helps the algorithm decide which tag makes sense based on the word itself.</li>
											</ul>

											<p>In essence, for each word, the algorithm selects the tag that maximizes the combination of how well it fits with the previous tag and how well it matches the current word.</p>
								
											<h3>The Viterbi Algorithm</h3>
											<p>Moving on to the Viterbi algorithm, things get a bit more intricate. This algorithm isn’t satisfied with just the immediate context; it looks at the entire sentence to decide the sequence of tags that makes the most sense. In my script, I implement this within the <code>viterbi_algorithm</code> function. It begins by filling a matrix with log probabilities for the first word across all possible tags. It then iterates over each word in the sentence, updating the matrix based on the transition probabilities, emission probabilities, and previously computed log probabilities. Finally, it backtracks from the last word to determine the most probable sequence of tags. To avoid the pitfalls of multiplying very small probabilities—which can cause numerical underflow—I use log probabilities instead. By transforming probabilities into log space, multiplication operations become additions, significantly mitigating the underflow risk. This approach is critical for the Viterbi algorithm's reliability, especially in processing long sentences where the product of many small probabilities would otherwise approach zero.</p>
											<p>Now let's break down the formula that the Viterbi algorithm uses to find the most likely sequence of tags for a given sentence:</p>

											<p>\[
											\hat{t}_1 \cdots \hat{t}_n = \arg\max_{t_1 \cdots t_n} \left( \prod_{i=1}^{n} P(t_i \mid t_{i-1}) \cdot P(w_i \mid t_i) \right) \cdot P(t_{n+1} \mid t_n)
											\]</p>

											<p>Here’s what the terms mean:</p>

											<ul>
												<li><strong>\(\hat{t}_1 \cdots \hat{t}_n\)</strong>: This represents the most likely sequence of tags for the sentence, from the first word to the last.</li>
												<li><strong>\(P(t_i \mid t_{i-1})\)</strong>: The <strong>transition probability</strong>, which tells us how likely the current tag \(t_i\) is, given the previous tag \(t_{i-1}\).</li>
												<li><strong>\(P(w_i \mid t_i)\)</strong>: The <strong>emission probability</strong>, which measures how likely it is that the current word \(w_i\) corresponds to the current tag \(t_i\).</li>
												<li><strong>\(t_0 = \langle \mathit{s} \rangle\)</strong>: This is the start-of-sentence marker.</li>
												<li><strong>\(t_{n+1} = \langle /\mathit{s} \rangle\)</strong>: This is the end-of-sentence marker.</li>
											</ul>

											<p>In simple terms, the Viterbi algorithm finds the sequence of tags that maximizes the product of the transition probabilities (from one tag to the next) and the emission probabilities (how well each word fits its tag) for the entire sentence. It also factors in the start and end markers to ensure the tagging process is coherent from beginning to end.</p>
											<h3>Individually Most Probable Tags</h3>
											<p>Lastly, the individually most probable tags algorithm represents the most detailed method in my toolkit. Implemented in the <code>most_probable_tags_algorithm</code> function, this algorithm leverages forward and backward probabilities to optimize the accuracy of POS tagging for each word based on the context provided by the other tags in the sentence. It relies on two support functions, <code>forward_algorithm</code> and <code>backward_algorithm</code>, which compute the forward and backward probabilities for each word being assigned each possible tag.</p>
											<p>Here’s how it works: The <code>most_probable_tags_algorithm</code> function first calculates the cumulative forward probabilities—those that move from the start of the sentence up to the previous word, factoring in both emission and transition probabilities. Then, it computes the backward probabilities, which run from the end of the sentence to the next word. Using the <code>logsumexp()</code> function, I ensure accurate summation of these probabilities. Finally, by multiplying these summed forward and backward probabilities together, the algorithm identifies the most probable tag for each word.</p>

											<p>In more formal terms, for each \(i\), the algorithm computes:</p>

											<p>\[
											\hat{t}_i = \arg\max_{t_i} \sum_{t_1 \cdots t_{i-1}, t_{i+1} \cdots t_n} \left( \prod_{i=1}^{n} P(t_i \mid t_{i-1}) \cdot P(w_i \mid t_i) \right) \cdot P(t_{n+1} \mid t_n)
											\]</p>

											<p>However, computing this directly would be inefficient. Instead, we break it down using forward and backward probabilities, similar to the Baum-Welch algorithm. The result is:</p>

											<p>\[
											\hat{t}_i = \arg\max_{t_i} \begin{array}{l} \sum_{t_1 \cdots t_{i-1}} \Bigl( \prod_{k=1}^{i} P(t_k \mid t_{k-1}) \cdot P(w_k \mid t_k) \Bigr) \cdot \\[2ex] \sum_{t_{i+1} \cdots t_n} \Bigl( \prod_{k=i+1}^{n} P(t_k \mid t_{k-1}) \cdot P(w_k \mid t_k) \Bigr) \cdot P(t_{n+1} \mid t_n) \end{array}
											\]</p>

											<p>This formula ensures that we not only consider the context of each word but also maximize the probability of each individual tag, making it a highly robust tagging method.</p>
										</section>
						
										
										<section class="section evaluation">
											<h2>Measuring Success: How The Algorithms Stacked Up</h2>
											<p>When evaluating the algorithms’ performance, I chose accuracy as the primary metric due to its ease of interpretability. Accuracy was calculated by comparing each algorithm’s predicted and true tags from the test sets. Here’s a snapshot of how each algorithm performed:</p>

											<p>Here’s a snapshot of how each algorithm performed:</p>
											<table border="1">
												<thead>
													<tr>
														<th>Language</th>
														<th>Accuracy % Eager</th>
														<th>Accuracy % Viterbi</th>
														<th>Accuracy % Individually Most Probable Tags</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>English</td>
														<td>88.6</td>
														<td>91.3</td>
														<td>88.6</td>
													</tr>
													<tr>
														<td>Swedish</td>
														<td>85.7</td>
														<td>90.2</td>
														<td>85.7</td>
													</tr>
													<tr>
														<td>Korean</td>
														<td>80.8</td>
														<td>79.2</td>
														<td>80.8</td>
													</tr>
												</tbody>
												
											</table>
									
											<p>Interestingly, the Individually Most Probable Tags algorithm did not outperform the Viterbi algorithm as initially anticipated. This outcome suggests potential implementation issues, likely stemming from the absence of transitions to the end-of-sentence marker in the forward algorithm and to the start-of-sentence marker in the backward algorithm. As a result, this algorithm was excluded from the final discussion.</p>
										</section>
										
										<section class="section discussion">
											<h2>Deep Dive: Analyzing Algorithm Performance and Linguistic Nuances</h2>

											<p>The Viterbi algorithm really shined for English and Swedish, offering accuracy boosts of 2.7% and 4.5% over the Eager algorithm. Why? It’s because Viterbi looks at the whole sentence when tagging, which works beautifully for languages with stricter word order. Swedish, too, benefits from this big-picture approach. But Korean was a different story. The Viterbi algorithm actually performed 1.6% worse than the Eager algorithm. Korean is what we call an “agglutinative language.” Think of words in Korean as building blocks — each word can stack multiple grammatical pieces onto itself. For example, a single word in Korean, like “먹었습니다” (meogeotseumnida), can include the verb root (“eat”), tense (“past”), and politeness level — all packed into one. This richness makes each word carry a lot of information, so even a simpler algorithm like Eager can figure out the tagging without needing to look at the entire sentence. On top of that, Korean’s word order is more flexible. For instance, “I apple ate” and “I ate apple” can both mean the same thing. This flexibility means Viterbi doesn’t gain as much from analysing the whole sentence — it’s almost like having extra tools you don’t really need adding unnecessary complexity.</p>


											<h3>Comparing Performance Across Languages.</h3>
											<p>English was the easiest for both algorithms to handle, achieving top accuracy scores of 91.3% with Viterbi and 88.6% with Eager. Why? English is a straightforward language when it comes to morphology — there aren’t many word endings or affixes to complicate things. For example, “run” stays the same whether you’re talking about running yesterday or tomorrow (aside from adding an auxiliary verb). On top of that, English has a rigid word order. Sentences can’t be rearranged without changing the meaning. This rigidity plays right into Viterbi’s strengths because it thrives on predictable structures.</p>

											<p>Swedish stood out with the biggest boost in accuracy when switching from Eager to Viterbi, improving by 4.5%. Swedish uses more word endings, or inflections, than English, which provide important clues about a word’s grammatical role. For instance, “katten” means “the cat,” indicating a definite noun, while “katts” means “cat’s,” showing possession. Viterbi’s ability to analyse the entire sentence allows it to piece together these inflectional clues and understand their relationships, giving it an advantage over the Eager algorithm, which focuses only on one word and its immediate context.</p>
											
											<h3>A note on the Universal Dependencies Treebank dataset</h3>
											<p>Interestingly, the Eager algorithm performed surprisingly well across all three languages, achieving accuracy scores of 88.6% for English, 85.7% for Swedish, and 80.8% for Korean. Why did it do so well? One reason is the Universal Dependencies Treebank, which simplifies tagging by grouping similar tags into just 17 universal categories. For example, instead of having dozens of tags for different verb forms, the Treebank uses one broad "verb" category. This simplification makes it easier for even a straightforward algorithm like the Eager to predict tags accurately.</p>
										
											<h2>My Reflections </h2>
											<h3>Key challenges</h3>

											<b>Cracking the Viterbi Algorithm</b>
											<p>Building the Viterbi algorithm was no easy feat. The mathematical complexity behind it made the development process challenging. Translating theoretical formulas into workable Python code took multiple iterations and a lot of deep dives into academic resources—lecture notes, textbook pseudocode, and online resources.</p>

											<b>Fine-Tuning the Individually Most Probable Tags Algorithm</b>
											<p>The Individually Most Probable Tags algorithm presented its own set of difficulties, especially since it wasn’t covered in our classes or textbooks. It demanded a solid grasp of mathematical principles to accurately integrate forward and backward probabilities into a predictive model. This algorithm still has room for improvement, and with further iterations, I believe it can achieve the results I initially expected.</p>
										</section>

										<section class="section findings">
											<h2>So What Did I Learn?</h2>
											<ul>
												<li><strong>Viterbi vs. Eager Algorithm:</strong>
													<ul>
														<li>The Viterbi algorithm performed better in Swedish and English, leveraging its ability to analyse entire sentence contexts effectively.</li>
														<li>The Eager algorithm slightly outperformed the Viterbi algorithm in Korean, relying on rich morphological cues encoded in individual words.</li>
													</ul>
												</li>
												<li><strong>Language Accuracy Rankings:</strong>
													<ul>
														<li>English had the highest accuracy for both algorithms, benefiting from simple morphology and rigid syntactic structure.</li>
														<li>Swedish showed significant improvement with the Viterbi algorithm (4.5% higher accuracy than Eager), likely due to its flexible syntax and moderate morphological complexity.</li>
														<li>Korean had the lowest accuracy across both algorithms due to its rich morphology and the limited utility of sentence-wide analysis in its syntax.</li>
													</ul>
												</li>
												<li><strong>Effect of Morphology:</strong>
													<ul>
														<li>Simpler morphology in English and Swedish facilitated easier tagging, as fewer word forms required recognition and categorisation.</li>
														<li>Korean’s agglutinative nature introduced a vast number of possible word forms, complicating the POS tagging process for both algorithms.</li>
													</ul>
												</li>
												<li><strong>Effect of Syntax:</strong>
													<ul>
														<li>Rigid syntax in English and Swedish, with strict word order defining grammatical roles, favoured the Viterbi algorithm’s sentence-wide analysis.</li>
														<li>Flexible syntax in Korean, which allows more variation in word order, diminished the effectiveness of the Viterbi algorithm, as sentence-wide context provided less additional information for POS tagging.</li>
													</ul>
												</li>
											</ul>
										</section>
										
										<section class="section conclusion">
											<h2>Future Work</h2>
											<p>Future work should prioritise the development of morphology-aware POS tagging models specifically designed for agglutinative languages. Perhaps this could involve incorporating morphological analysers to decompose words into their constituent morphemes, providing valuable information about grammatical function.  Additionally, exploring methods that can effectively capture long-range dependencies in Korean, despite its flexible word order, would be a valuable direction for future research. Such advancements could significantly improve POS tagging accuracy for Korean and other agglutinative languages such as Turkish, Finnish and Japanese.</p>
										</section>

										<section class="section try-it-out"> 
											<h2>Try It Out</h2>
											<p>Curious to see how the algorithms perform in action? Pull the repository and try it for yourself.</p>
											<a href="https://github.com/emma-horton/PartsOfSpeech/blob/main/README.md" class="btn">Get the Code on GitHub</a>
										</section>

										<section class="section additional">
											<h2>Want to find out more?</h2>
											<p>For a deeper dive into the Viterbi algorithm and its applications in POS tagging, check out the recommended books below:</p>
											<div class="recommended-books">
												<!-- <h3>Recommended Books</h3> -->
												<div class="book">
													<img src="images/speech_and_nlp.jpeg" alt="Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition.<">
													<h4>Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition.</h4>
													<p>Jurafsky, D. and Martin, J.H.</p>
													<a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">Access online</a>
												</div>
												<div class="book">
													<img src="images/nlp_with_python.jpeg" alt="Natural language processing with Python">
													<h4>Natural language processing with Python</h4>
													<p>Bird, S., Klein, E. and Loper, E.</p>
													<a href="https://www.nltk.org/book/">Access online</a>
												</div>
												<!-- Add more books as needed -->
											</div>
										</section>

									</div>
								</body>
							</section>

					</div>

				<!-- Footer -->
					<!-- <footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section> -->
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>1234 Somewhere Road #87257<br />
								Nashville, TN 00000-0000</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(000) 000-0000</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="#">info@untitled.tld</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>
